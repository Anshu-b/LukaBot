{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(\"luka_bot.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining constants (in terms of meters)\n",
    "GRAVITY = 9.81\n",
    "HOOP_X = 7.24 \n",
    "HOOP_Y = 3.05  \n",
    "BASKET_RADIUS = 0.2286 \n",
    "BALL_RADIUS = 0.12065 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feet_to_meters(feet):\n",
    "    '''\n",
    "    Helper function to convert feet to meters\n",
    "    '''\n",
    "    return feet * 0.3048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class defining the LukaBot environment\n",
    "class LukaBotEnv(gym.Env):\n",
    "    '''\n",
    "    Custom Environment that follows gym interface for LukaBot shooting simulation.\n",
    "    Observation space: [Distance to hoop (m), final x position (m), final y position (m), shots taken, successful shots]\n",
    "    Observation space limited to [0, 10] m for distance and x position, [0, 5] m for y position, [0, 10] for shots taken and successful shots.\n",
    "    Action space: [Velocity (m/s) and angle (degrees)]\n",
    "    Action space limited to [5, 30] m/s for velocity and [20, 80] degrees for angle.\n",
    "    Each episode consists of 10 shot attempts.\n",
    "    '''\n",
    "    def __init__(self, player_height_ft=6.5): # Luka is 6'6\n",
    "        super(LukaBotEnv, self).__init__()\n",
    "\n",
    "        # Training trackers\n",
    "        self.current_episode = 0\n",
    "        self.total_rewards = 0\n",
    "        self.successful_shots = 0\n",
    "        self.shots_taken = 0 \n",
    "        self.max_shots_per_episode = 10\n",
    "\n",
    "        # tracking shot and state information\n",
    "        self._last_shot_info = {}\n",
    "        self.state_history = []\n",
    "\n",
    "        # Environment attributes\n",
    "        self.release_point_height = feet_to_meters(player_height_ft + 1) \n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0, 0]),  high=np.array([10, 10, 5, self.max_shots_per_episode, self.max_shots_per_episode]), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=np.array([5, 20]), high=np.array([30, 80]), dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        '''\n",
    "        Reset the environment to an initial state.\n",
    "        This method is called at the beginning of each episode.\n",
    "        Set the state to the initial values, reset the shot counters, and log the episode number.\n",
    "        '''\n",
    "        self.current_episode += 1\n",
    "        self.total_rewards = 0\n",
    "        self.successful_shots = 0\n",
    "        self.shots_taken = 0  \n",
    "\n",
    "        if self.current_episode % 1000 == 0:\n",
    "            logger.info(f\"Episode {self.current_episode}\")\n",
    "            logger.info(f\"Total Rewards: {self.total_rewards}\")\n",
    "            logger.info(f\"Successful Shots: {self.successful_shots}\")\n",
    "\n",
    "        self._last_shot_info = {\n",
    "            'velocity': None,\n",
    "            'angle': None,\n",
    "            'distance': None,\n",
    "            'height': None,\n",
    "            'reward': None\n",
    "        }\n",
    "        self.state = np.array([HOOP_X, 0, self.release_point_height, self.shots_taken, self.successful_shots], dtype=np.float32)\n",
    "        return self.state, {}\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_reward(self, x, y):\n",
    "        '''\n",
    "        Helper function to calculate the reward based on the shot's outcome.\n",
    "        '''\n",
    "        if y <= 0:\n",
    "            return -5 # Dynamic penalty for missing the hoop\n",
    "\n",
    "        x_distance_to_hoop = abs(x - HOOP_X)\n",
    "        distance_to_hoop = np.sqrt((x - HOOP_X)**2 + (y - HOOP_Y)**2)\n",
    "        if x_distance_to_hoop <= (BASKET_RADIUS - BALL_RADIUS) and abs(y - HOOP_Y) <= 0.03:\n",
    "            if self.successful_shots > 0 and self._last_shot_info.get('reward', 0) >= 10:\n",
    "                bonus = 2 # bonus for consecutive makes\n",
    "            else:\n",
    "                bonus = 0 \n",
    "            return 10 + bonus\n",
    "        \n",
    "        # Partial Reward if the shot is close but doesn't go in\n",
    "        if distance_to_hoop < 0.1:  \n",
    "            return 7\n",
    "        elif distance_to_hoop < 0.2:  \n",
    "            return 5\n",
    "        elif distance_to_hoop < 0.3:  \n",
    "            return 3\n",
    "        elif distance_to_hoop < 0.5:  \n",
    "            return 2 \n",
    "        elif distance_to_hoop < 1.0:  \n",
    "            return 1  \n",
    "\n",
    "        # Negative reward for ball far away from the hoop\n",
    "        return -15\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Simulate a shot based on the action taken (velocity and angle).\n",
    "        The shot is simulated for a maximum of 3 seconds or until the ball hits the ground or goes through the hoop.\n",
    "        The method returns the new state, reward, done flag, and additional info about the last shot.\n",
    "        The state is updated to reflect the final distance to the hoop, final x and y positions, shots taken, and successful shots.\n",
    "        '''\n",
    "        velocity, angle = action\n",
    "        logger.info(f\"Action received: {action}\")\n",
    "        angle = np.deg2rad(angle)  \n",
    "        v_x = velocity * np.cos(angle)  # Horizontal velocity\n",
    "        v_y = velocity * np.sin(angle)  # Vertical velocity\n",
    "        t = 0  \n",
    "        dt = 0.01 \n",
    "        max_time = 3  \n",
    "        x = 0\n",
    "        y = self.release_point_height\n",
    "\n",
    "        # Simulate the shot, stop if ball hits the ground or scores\n",
    "        while t < max_time:\n",
    "            x = v_x * t  \n",
    "            y = self.release_point_height + v_y * t - 0.5 * GRAVITY * t**2  \n",
    "            if y <= 0: \n",
    "                break\n",
    "            elif abs(x - HOOP_X) <= (BASKET_RADIUS - BALL_RADIUS) and abs(y - HOOP_Y) <= 0.03: \n",
    "                break\n",
    "            t += dt\n",
    "\n",
    "        reward = self.calculate_reward(x, y)\n",
    "        self.total_rewards += reward\n",
    "\n",
    "        # Update successful shots and shots taken\n",
    "        if reward >= 10:  \n",
    "            self.successful_shots += 1\n",
    "        self.shots_taken += 1\n",
    "\n",
    "        # Check if episode is done (10 shot attempts)\n",
    "        done = (self.shots_taken >= self.max_shots_per_episode)\n",
    "\n",
    "        # Update last shot info\n",
    "        self._last_shot_info.update({\n",
    "            'velocity': velocity,\n",
    "            'angle': np.rad2deg(angle),\n",
    "            'distance': x,\n",
    "            'height': y,\n",
    "            'reward': reward\n",
    "        })\n",
    "\n",
    "        logger.info(f\"Shot {self.shots_taken}/{self.max_shots_per_episode} Details: \"\n",
    "                    f\"Velocity={velocity:.2f}m/s, \"\n",
    "                    f\"Angle={np.rad2deg(angle):.2f}°, \"\n",
    "                    f\"Distance={x:.2f}m, \"\n",
    "                    f\"Height={y:.2f}m, \"\n",
    "                    f\"Reward={reward}\")\n",
    "\n",
    "        # After the shot, reset the ball's position to the starting point for the next shot\n",
    "        if not done:\n",
    "            distance_to_hoop = HOOP_X  \n",
    "            x = 0  \n",
    "            y = self.release_point_height  \n",
    "        else:\n",
    "            distance_to_hoop = np.sqrt((x - HOOP_X)**2 + (y - HOOP_Y)**2) \n",
    "\n",
    "        self.state = np.array([distance_to_hoop, x, y, self.shots_taken, self.successful_shots], dtype=np.float32)\n",
    "        self.state_history.append(self.state)\n",
    "        logger.info(f\"Updated state: {self.state}, Reward: {reward}, Done: {done}\")\n",
    "        return self.state, reward, done, False, self._last_shot_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_visualize(total_timesteps=50000):\n",
    "    \"\"\"\n",
    "    Train the SAC agent and visualize shot trajectories.\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = LukaBotEnv()\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False)\n",
    "    \n",
    "    # Train SAC model\n",
    "    model = SAC(\"MlpPolicy\", env, learning_rate=0.0003, buffer_size=100000, batch_size=512, tau=0.005, gamma=0.99, ent_coef=\"auto\", verbose=1)\n",
    "    model.learn(total_timesteps=total_timesteps, log_interval=100)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_agent(env, model, episodes=10)\n",
    "\n",
    "\n",
    "def test_agent(env, model, episodes=10):\n",
    "    \"\"\"\n",
    "    Test the trained SAC agent and visualize shot trajectories.\n",
    "    \"\"\"\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset() \n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        episode_successful_shots = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)  # Use deterministic actions for testing\n",
    "            print(f\"Predicted Action: {action}\")\n",
    "            \n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            reward = reward[0]  # Shape (1,) -> scalar\n",
    "            done = done[0]      # Shape (1,) -> scalar\n",
    "            info = info[0]      # Shape (1,) -> dict\n",
    "            episode_rewards += reward\n",
    "\n",
    "            if reward >= 10:  # scores\n",
    "                episode_successful_shots += 1\n",
    "            print(f\"Observation After Action: {next_obs}, Reward: {reward}\")\n",
    "            print(f\"\\nShot {int(next_obs[0][3])} in Episode {episode + 1}:\")\n",
    "            print(f\"Shot Velocity: {info['velocity']:.2f} m/s\")\n",
    "            print(f\"Shot Angle: {info['angle']:.2f} degrees\")\n",
    "            print(f\"Distance: {info['distance']:.2f} m\")\n",
    "            print(f\"Height: {info['height']:.2f} m\")\n",
    "            print(f\"Reward: {reward}\")\n",
    "            \n",
    "            # Visualize the shot\n",
    "            plot_shot(info['velocity'], info['angle'])\n",
    "            obs = next_obs\n",
    "        \n",
    "        # Log episode summary\n",
    "        print(f\"\\nEpisode {episode + 1} Summary:\")\n",
    "        print(f\"Total Rewards: {episode_rewards}\")\n",
    "        print(f\"Successful Shots: {episode_successful_shots}\")\n",
    "\n",
    "\n",
    "def plot_shot(velocity, angle):\n",
    "    \"\"\"\n",
    "    Create a 2D visualization of a basketball shot trajectory.\n",
    "    \"\"\"\n",
    "    # Simulation parameters\n",
    "    release_height = feet_to_meters(7.5)\n",
    "    angle_rad = np.radians(angle)\n",
    "    t_values = np.linspace(0, 2, num=100)\n",
    "    \n",
    "    # Calculate trajectory\n",
    "    x_values = velocity * np.cos(angle_rad) * t_values\n",
    "    y_values = (release_height + velocity * np.sin(angle_rad) * t_values - 0.5 * GRAVITY * t_values**2)\n",
    "    \n",
    "    # Plot trajectory\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_values, y_values, label=f\"v={velocity:.2f}, θ={angle:.2f}°\")\n",
    "    plt.scatter([HOOP_X], [HOOP_Y], color='red', label='Hoop')\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(0, 5)\n",
    "    plt.xlabel(\"Distance (m)\")\n",
    "    plt.ylabel(\"Height (m)\")\n",
    "    plt.title(\"Basketball Shot Trajectory\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
